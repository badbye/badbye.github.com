---
layout: post
title: "爬虫利器"
date: 2016-11-29 17:42:06 -0700
comments: true
---


做过一小段时间的爬虫，大部分网站都是直接下载网页或者找到后端的API接口，连Cookie验证的都很少。一旦有验证，虽然也有对应的套路，但相对来说也麻烦的很。

目前小菜鸟所掌握的无非就是以下的套路：

1. 在Chrome开发者模式下找到数据来源的API，查看请求中的Cookie或者Token

2. 在所有请求里，追本溯源找这些Cookie或Token是在哪一步返回的

3. 模拟请求

通常来说都会先上第三步，有错误再走前两步。其中第三步模拟请求的很简单也很无聊，基本就是配置参数:

```python
requests.get(url, headers, coookie, **kargs)
```

本文介绍的利器就是解放第三步的无聊工作的。利器组合：Chrome + [CurlWget](https://chrome.google.com/webstore/detail/curlwget/jmocjfidanebdlinpbcdkcmgdifblncg) + [uncurl](https://github.com/spulec/uncurl)。

Chrome不必多说。`CurlWget`是一个扩展，也称"Copy as URLs"，一键模拟curl的参数(curl是终端下的浏览器)。`uncurl`是一个Python包，将curl的命令参数转换成requests的命令。

一个简单的例子，打开Chrome的开发者模式，点开微博首页。查看Network选项卡：

<img class="img-responsive" src='{{site.url}}/images/curl.png'>

鼠标这么一点，粘贴到终端，一个回车下去就能看到你微博首页的内容了。用uncurl工具很容易就转成requests的命令。

```bash
$ uncurl "拷贝进来curl的命令"
requests.get("http://weibo.com/u/xxxxxxxxx/home?topnav=1&wvr=5",
    headers={
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, sdch",
        "Accept-Language": "zh-CN,zh;q=0.8,en;q=0.6,de;q=0.4",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "DNT": "1",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36"
    },
   cookies={
        "xxxxxx": "xxxxx",  # 我是不会暴露自己的cookie的...
        "_s_tentry": "login.sina.com.cn",
        "wvr": "6"
    },
)
```

随意感受一下吧。

----------------------------------
2017.2.19: 更新，R语言用户也可以使用[curl2r](https://github.com/badbye/curl2r)这个小工具把curl的命令转化成R语言的命令。

安装并配置完之后，就可以试试

```bash
$ curl2r 拷贝进来curl的命令  # 此处不要用双引号括住!


library(httr)
GET("https://www.baidu.com/",
    add_headers(c(Pragma = "no-cache",
        DNT = "1", `Accept-Encoding` = "gzip, deflate, sdch, br",
        `Accept-Language` = "zh-CN,zh;q=0.8,en;q=0.6,de;q=0.4",
        `Upgrade-Insecure-Requests` = "1",
        `User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",
        Accept = "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        `Cache-Control` = "no-cache",
        Connection = "keep-alive")),
    set_cookies(XXX))
```

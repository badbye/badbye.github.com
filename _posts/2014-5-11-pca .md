---
layout: post
title: "Compute Principle Component in R"
date: 2014-05-11 16:25:06 -0700
comments: true
---

主成分的目的是降维,把原本相关的众多变量压缩成少数的几个综合变量,使得这几个综合变量既包含原始变量的主要信息,又互不相关.

*互不相关*主要是指线性无关,*信息*则用方差来表示.方差越大,包含的信息越多.最初一直不明白为什么方差能表示信息,直到读了[信息论](http://www-nlp.stanford.edu/IR-book/).

主成分的实施主要有两种方式,一个是基于协方差阵的特征分解,另一个是基于设计矩阵的奇异值分解(singular value decomposition).

## 特征分解

记$\Sigma_{p*p}$是原始变量$X_{m*p}(m > p)$的协方差阵,$\lambda_1 > \lambda_2 > \cdots > \lambda_p$是$\Sigma$的特征根,$U = [u_1,u_2 \cdots u_p]$中每列是相应的单位特征向量,由线性代数知识知道这些特征向量都是相互正交的.

于是第一个主成分$Y_1$就是: 
$$\begin{aligned} 
Y_1 &= Xu_1 = U_{11}X_1 + U_{21}X_2 + \cdots + U_{p1}X_p \\ 
var(Y_1) &= var(Xu_1) \\ 
         &= u_1^T var(X)u_1 \\ 
         &= u_1^T \Sigma u_1 \\ 
         &= u_1^T \lambda_1 u_1 \\ 
         &= \lambda_1 
\end{aligned}$$

依次的有,$Y_i =Xu_i =U_{1i}X_1 + U_{2i}X_2 + \cdots + U_{pi}X_p$,相应的方差就是$\lambda_i$,所以有主成分的方差越来越小,且$cov(Y_i, Y_j) = cov(Xu_i, Xu_j) =0(i \neq j)$即不同的成分之间线性无关.

用向量的形式表示就是$Y = X*U$,其中$U$是协方差阵$\Sigma$的特征向量.最后的主成分只是对设计矩阵做了一个线性变换,使得各成分互不相关且方差递减.

又方差越大信息量越大,于是用$\frac{\lambda_i}{\lambda_1 + \lambda_2 + \cdots + \lambda_p}$表示第$i$个成分的方差贡献率,即包含总的信息量的百分比.取前几个总贡献率超过90%(或其他阈值)的主成分,剩余的丢掉,就达到了降维的目的.

另外,当数据量纲不同时,通常要做标准化处理,不然量纲大的变量方差大,成分会主要集中在数量级比较大的变量上.从计算的角度看,可以用相关系数矩阵替换协方差阵做特征分解.

根据以上计算方式,自己编写一个主成分的函数myPcaEigen,返回最终的主成分:

```R
myPcaEigen = function(X, useCor = F) {
    # useCor: decompose Correlation matrix, default is FALSE, otherwise use
    # Covariance instead
    X = as.matrix(X)
    if (useCor) 
        u = eigen(cor(X))$vectors else u = eigen(var(X))$vectors
    Y = scale(X, scale = useCor) %*% u
    colnames(Y) = paste0("Comp.", 1:ncol(Y))
    Y
}
```

R语言中内置的函数princomp*用来做基于特征分解的主成分.参数*cor*表示用相关系数矩阵做分解,相当于*myPcaEigen*里的*useCor,默认为FALSE.返回的列表里,*scores*表示最终的主成分.

可自行运行以下代码，对比自编函数和自带函数的返回结果，应该是完全一致的。


```R
head(myPcaEigen(USArrests, useCor = F), 3) # use Covariance
head(princomp(USArrests, cor = F)$scores, 3) # the same 
```

```R
head(myPcaEigen(USArrests, useCor = T), 3) # use Corralation
head(princomp(USArrests, cor = T)$scores, 3) # the same
```


从协方差阵出发,计算出的结果一样,偶尔可能有符号上的差别,不影响进一步的建模分析;从相关系数阵出发,计算结果有些许出入,准确的说相差了一个常数$\sqrt\frac{49}{50}$.

```R
head(princomp(USArrests, cor = T)$scores * sqrt(49/50), 3)
# the same with previously one
```

这样就与自编的函数结果一致了.这是因为*princomp*函数中方差的计算是除以$N$:

> Note that the default calculation uses divisor N for the covariance matrix.

而在自编函数中计算$var$时:

> The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations.

上述引用可以在函数的帮助文档中查到.**但为什么基于协方差的计算结果一致,基于相关系数的就有偏差**?

(1)基于相关系数的计算,相当于对设计矩阵$X$做了中心化和标准化处理之后,再进行翻转;而基于协方差的计算,是只对设计矩阵$X$做了中心化处理.

(2)不论除以$N$还是$N-1$,只相差一个常数因子的矩阵分解得到的单位特征向量$U$是一致的.

(3)基于协方差的计算,中心化后的设计矩阵和特征向量都一致,最终结果也一样;基于相关系数的计算,在标准化时*myPcaEigen*中计算方差时除以$N-1$,而*princomp*中除以$N$,一个常数因子的差异便因此而来.

注:

- 中心化: $x_i = x_i - \bar{x}$
- 标准化: $x_i = \frac{x_i - \bar{x}}{var(x)}$
- 其中,$\bar{x} = \frac{1}{m}\sum_{j=1}^{m}x_j$



## 奇异分解(SVD)


首先介绍一下SVD分解,任意的矩阵$A$都可以分解成三个矩阵的乘积: \[ A_{m*n} = U_{m*m} * \Sigma_{m*n} * V^{T}_{n*n} \] 各个矩阵的维度都在下标中给出,另外还有三个性质:

$U$的列向量是矩阵$AA^{T}$的单位正交特征向量;
$V$的列向量是矩阵$A^{T}A$的单位正交特征向量;
$\Sigma$中每个对角元都是$A$的特征值,且从上到下依次变小,其他元素均为0;
因为$U$和$V$的列向量都是单位正交的,所以有$U^{T}U = I_{m}$和$V^{T}V = I_{n}$.

假定设计矩阵$X_{m*p}$中每行表示一个样本,每列表示一个变量.而且每个变量都经过中心化处理,均值为0.那么$\Sigma = \frac{1}{m-1}*X^{T}X$便是变量的协方差阵,与矩阵$X^{T}X$的单位特征向量一样.

于是对$X$进行SVD分解,得到$X_{m*p} = U_{m*m} * \Sigma_{m*p} * V^{T}_{p*p}$,其中矩阵$V$的列向量便是$X^{T}X$的单位特征向量,主成分变换就是$X*V$.

接下来自己编写一个SVD分解的主成分函数,同样只返回最终的主成分:

```R
myPcaSvd = function(X, useCor = F) {
    # useCor: decompose Correlation matrix, default is FALSE, otherwise use
    # Covariance instead
    X = scale(as.matrix(X), scale = useCor)
    u = svd(X)$v
    Y = X %*% u
    colnames(Y) = paste0("Comp.", 1:ncol(Y))
    return(Y)
}
```

$R$语言中也有内置的基于SVD分解的函数prcomp,返回值中*x*是最终的主成分.

```R
### use Covariance
head(myPcaSvd(USArrests), 3)
head(prcomp(USArrests)$x, 3)

### use Correlation
head(myPcaSvd(USArrests, T), 3)
head(prcomp(USArrests, scale = T)$x, 3)
```

另外,*prcomp*函数在计算方差时是除以$N-1$的,帮助里有写:

> Unlike princomp, variances are computed with the usual divisor N - 1.

## 总结
主成分在$R$语言中计算至此终于搞清了,其他方差贡献率啊因子载荷啊什么的全都忽略了,明白计算过程的话这一切都没问题.初学者常见的问题就是在$SPSS$里和$R$里同时做主成分,得到结果相差蛮大,不知道该信哪个.这就逼着用户更深入的了解模型,是否要对数据进行标准化处理得到的结果是完全不同的.$R$语言的好处就是几行代码就可以自己编写一个函数来验证,实现的过程也是对计算过程的更深理解.

另外,出人意料的差错,比如*princomp*基于相关系数计算的不一致,一般都能在帮助文档里找到原因.如果作者有良心的话.

## 题外话
很久之前就想写写主成分分析的东西,当年初学$R$语言的时候也对计算结果表示困惑.后来刚开笔,就看到了统计之都这篇[奇异值分解和图像压缩](http://cos.name/2014/02/svd-and-image-compression/),出自轩神.看完自惭形秽,不敢再动笔…

直到在[deep learning](http://deeplearning.stanford.edu/wiki/index.php/Implementing_PCA/Whitening)里再次邂逅主成分,发现这里的计算方法是竟然是对协方差进行SVD分解?!超越了我的认知范围,一度怀疑是不是写错了?但是运行下来结果也很靠谱,终于不能再懒,稍微动手推一下:

假定$u_i$和$\lambda_i$是协方差阵$\Sigma$的第$i$个特征根和特征向量,即有$\Sigma u_i= \lambda_i u_i$.

由于$\Sigma = \Sigma^{T}$.所以$\Sigma^{T}\Sigma = \Sigma\Sigma^{T}$,而且$\Sigma^{T}\Sigma u_i = \Sigma^{T}*\lambda_i*u_i=\lambda_i^2 u_i$.

所以$\Sigma$和$\Sigma^{T}\Sigma$的特征向量是一样的,但特征根不同.于是对$\Sigma$做SVD分解得到的$U$和$V$两个矩阵是一样的,每列都是$\Sigma\Sigma$的特征向量,也是$\Sigma$的特征向量.

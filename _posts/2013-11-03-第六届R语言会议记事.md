---
layout: post
title: "第六届R语言会议记事"
date: 2013-11-03 16:25:06 -0700
comments: true
---

近两年着实走了不少地方，见了许多美丽风景，做了些许二逼趣事，但又似乎像背英语一样过目即忘。其实也不能说是忘了，只是一直没去想而已。有必要写写日记。

继上次的古典骇客音乐会，这次的R语言会议是第二个让我感受到魔都大都市气息的地方。学术界和业界大牛云集，终于见到统计之都里李舰，魏太云，cloudy，魔王等真人风采。可惜两天报告下来能消化的倒不多。

## 林智仁教授
之前在会议安排里看到几篇报告的摘要都是英文的，顿时觉得会不会太高端了些？第一个报告这位林教授连脚注里的简介都是英文的，以至于根本没细看。后来到现场才意识到这位就是libsvm包的开发者，瞬间崇拜万分。

林教授以自身经历分享了编写机器学习和数据挖掘软件的经验，对我这毛头小子基本是没什么帮助的。通篇演讲也是中英混杂，好在之前有在coursera和网易修过机器学习的课，竟然都听懂了，不由的小得意一下。中间有讲到些模型改进的措施，比如数据的预处理，核函数的选择以及参数调整有借鉴之处。想起本科论文时的文本分类只简单做了稀疏矩阵的删减和核函数的选择，参数没调就扔进去跑了。结果训练集准确率98%，测试集只有85%左右，严重过拟合就那么交了，反正老师看不懂。真是惭愧惭愧…

## 何通
这位就是当年在果壳写篇文章被老师拿去当考题的大神—严酷的魔王了，也是直到他的报告才知道魔王真相。分享了豆瓣标签数据的整理和分析，整场报告围绕一个主题层层递进，最符合我的审美观。

豆瓣里音乐，电影等的标签是由用户自己打的，于是各种奇葩的标签层出不穷，需要先机器预处理再由工作人员手工操作，一来给产品加标签，二来给用户加标签，用于后期的推荐系统。

基本的去噪分词倒是不难做的。但分词后数据量仍然不小，人工很难处理，于是就有了下边很有趣的基于关联规则的近义词处理。比如"惊悚"，“恐怖"和"悬疑"等标签表达意义相近，可以由机器直接转化为一个标签，省去不少麻烦。具体的算法大概就是计算一个条件概率，比如在对一部电影的描述中如果\( P(词B出现|词A出现) \)很大，就意味着A和B是对同一类事物的描述，含义相近。最后设置阈值使得大于某个值的条件概率所对应的两个词归为近义词。但是A和B相近同样意味着B和A相近，那相应的还有另一个条件概率\( P(词A出现|词B出现) \)呢，这个怎么处理倒没介绍，如果是我的话会选两者中比较大的概率，有机会要折腾一下。

本来以为到这一步就结束，没想到后续还有一个标签的层次划分，给标签划分层次，比如"日本动漫"这一个标签下再划分有"热血”，“励志"等标签，这样一个类似树状图的东西，但各颗树又相互交叠，比如"军事电影"里同样会有"热血”，“励志"这样的标签，这样一层一层的划分出来，越上层的标签越抽象，越底层的越具体，之后可以尝试更有趣的推荐。对"励志"的"日本动漫"爱好者推荐其他的日本动漫，或者"军事电影"中励志电影，甚至跨越影视的界限推介些励志的书籍。相关的算法同样是计算条件概率的问题，如果\( P(A|B) \)很大但\( P(B|A) \)却很小就说B从属于A，因为B出现时A有很大概率出现反之却不然，说明A包含B。印象中就是这个样子了，总感觉哪里错了的样子，回头再瞅瞅幻灯片吧。

## 李舰
舰叔出品，必属精品。

这次大会介绍了他的文本挖掘新包:tmcn。相对于之前的Rweibo和Rwordseg来说，新包发布好低调，之前竟然没听说一点消息。舰叔的博客也还没更新，附个安装方法:

```r
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
```

此包主要用于中文的文本挖掘，提供了一些我觉得很有用的函数:

- toUTF8
无论是在windows里，还是linux下我都被中文的各种编码蹂躏过。网页里的编码"GBK”，“GBK2312"和"BIG5"等各种各样都有，知道它们是何种编码还好，可以用*iconv*函数来转换。问题是大部分情况下系统不能识别编码我们也不知道，只能挨个尝试。现在好了，*toUTF8*把一切中文字符转化为UTF8，任它千万变化，我只一招制敌!世界终于清静了…
- toPinyin
这个函数把汉字转化为拼音，之所以有趣是因为另一个包*ggmap*里有一个*geocode*函数可以根据地址得到经纬度坐标，但变量只能是拼音，之前一直找把汉字转化为拼音的函数，现在终于有了。终于可以在地图上毫无压力的标注城市了。

- stopwordsCN
包含了503个中文的停止词，除了感动我还能说些什么？

另外这个包与tm包相比更方便使用，其数据格式还是原本的向量，矩阵，列表和数据框等，没有建立新的S3类。回想起第一次用tm包连建立和查看语料库都得翻一遍帮助文档，真是太辛酸了。

## 陈逸波
经常在统计之都的求助帖里见到一位ID名为波波头一头的大神回复"是这样嘛？(略去一行代码)”，然后我默默的打开R看看这个函数的帮助，欣喜的发现还有这么好使的函数。

见到本人已经比微博和统计之都里的头像瘦多了，大概是经常跑步的缘故。回到正题。波波前辈分享了在kaggle实战的经验，在数据属性都不知道的前提下达到91%的预测准确率，受益良多。虽然说简单些就是了解数据，特征选取，建模和改进的过程，但过程之艰辛需要不断的尝试，假设，实验。终于稍微明白些什么叫对数据的敏感了。

## 其他
第二位来自台湾的林祯舜教授，讲述了大数据对统计教育的影响，真应该上传视频让所有统计老师都听听的…

ebay专场讲述了R在ebay的应用，包括了Rhipe和Rhadoop的介绍，终于知道这俩单词怎么发音了，同时也好像搭建个集群玩玩，嘛时候把宿舍四台电脑连起来跑跑2000W？另:感觉ebay的人都有种气场，如此牛逼的跨国公司就是这么的不一样么？附个做报告时被拍最多的ebay美女照片。 [不好意思，图片被我弄丢了╮(╯_╰)╭]

会议之前就被echarts震撼到，各种交互式操作实在是忽悠人的最佳利器。可惜跟D3一样是个js库，想起暑假在菲律宾还学过几招D3，现在只能差不多看懂代码但写不出来了。好在魏太云和周扬开发了R的接口recharts，在R里也能方便的画出来了，还能跟shiny和knitr结合。前两天刚用shiny写完的web应用又要改了。

多元作业的paper还没读，linux下的latex又要重新学。听两天报告就又给自己找这么多事儿，真是能自娱自乐。禁不住又要一声悠远深长的叹息，唉~~~